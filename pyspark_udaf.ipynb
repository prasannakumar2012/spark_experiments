{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import col, count, rand, collect_list, explode, struct, count\nfrom pyspark.sql.functions import pandas_udf\n\n# df = spark.range(0, 10 * 1000 * 1000).withColumn('id', (col('id') / 1000).cast('integer')).withColumn('v', rand())\ndf = spark.range(0, 10 * 1000).withColumn('id', (col('id') / 1000).cast('integer')).withColumn('v1', rand()).withColumn('v2', rand())\ndf.cache()\ndf.count()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndisplay(df.groupBy(\"id\").agg(corr(col(\"v1\"),col(\"v2\"))))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import pandas as pd\nfrom scipy import stats\ngroup_column = 'id'\nfirst_col = 'v1'\nsecond_col = 'v2'\nschema = df.select(group_column, first_col).schema\n\n@pandas_udf(schema)\n# Input/output are both a pandas.DataFrame\ndef group_corr(pdf):\n    group_key = pdf[group_column].iloc[0]\n    temp_corr = stats.pearsonr(pdf[first_col],pdf[second_col])[0]\n    return pd.DataFrame([[group_key] + [temp_corr]], columns=[group_column] + [\"corr_val\"])\n  \ndisplay(df.groupby(group_column).apply(group_corr))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df.groupby(group_column).apply(group_corr).explain()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.groupBy(\"id\").agg(corr(col(\"v1\"),col(\"v2\"))).explain()"],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"pyspark_udaf","notebookId":2756581186007367},"nbformat":4,"nbformat_minor":0}
