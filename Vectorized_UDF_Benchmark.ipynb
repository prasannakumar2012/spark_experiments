{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import col, count, rand, collect_list, explode, struct, count"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf\n\ndf = spark.range(0, 10 * 1000 * 1000).withColumn('id', (col('id') / 1000).cast('integer')).withColumn('v', rand())\n\ndf.cache()\ndf.count()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["@udf(\"double\")\ndef plus_one(v):\n    return v + 1\n\n%timeit df.withColumn('v', plus_one(df.v)).agg(count(col('v'))).show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["@pandas_udf(\"double\")\ndef vectorized_plus_one(v):\n    return v + 1\n\n%timeit df.withColumn('v', vectorized_plus_one(df.v)).agg(count(col('v'))).show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import pandas as pd\nfrom scipy import stats\n\n@udf('double')\ndef cdf(v):\n    return float(stats.norm.cdf(v))\n\n%timeit df.withColumn('cumulative_probability', cdf(df.v)).agg(count(col('cumulative_probability'))).show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import pandas as pd\nfrom scipy import stats\n\n@pandas_udf('double')\ndef vectorized_cdf(v):\n    return pd.Series(stats.norm.cdf(v))\n\n\n%timeit df.withColumn('cumulative_probability', vectorized_cdf(df.v)).agg(count(col('cumulative_probability'))).show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import Row\n@udf(ArrayType(df.schema))\ndef substract_mean(rows):\n    vs = pd.Series([r.v for r in rows])\n    vs = vs - vs.mean()\n    return [Row(id=rows[i]['id'], v=float(vs[i])) for i in range(len(rows))]\n  \n%timeit df.groupby('id').agg(collect_list(struct(df['id'], df['v'])).alias('rows')).withColumn('new_rows', substract_mean(col('rows'))).withColumn('new_row', explode(col('new_rows'))).withColumn('id', col('new_row.id')).withColumn('v', col('new_row.v')).agg(count(col('v'))).show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["@pandas_udf(df.schema)\n# Input/output are both a pandas.DataFrame\ndef vectorized_subtract_mean(pdf):\n\treturn pdf.assign(v=pdf.v - pdf.v.mean())\n\n%timeit df.groupby('id').apply(vectorized_subtract_mean).agg(count(col('v'))).show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(df.orderBy(\"id\"))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndisplay(df.groupBy(\"id\").agg(mean(\"v\")).orderBy(\"id\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(df.groupby('id').apply(vectorized_subtract_mean).orderBy('id'))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"Vectorized UDF Benchmark","notebookId":2565804904784160},"nbformat":4,"nbformat_minor":0}
